{
    "publications": {
        "publication1": {
            "title": "Speech-Augmented Cone-of-Vision for Exploratory Data Analysis",
            "venue":"ACM CHI Conference on Human Factors in Computing Systems (CHI 2023)",
            "authors": [
                "Riccardo Bovo",
                "Daniele Giunchi",
                "Ludwig Sidenmark",
                "Joshua Newn",
                "Hans Gellersen",
                "Enrico Costanza",
                "Thomas Heinis"
            ],
            "links": {
                "acm": "https://dl.acm.org/doi/abs/10.1145/3544548.3581283",
                "youtube": "https://www.youtube.com/watch?v=6meJveJe_mY",
                "github": "https://github.com/Collaborative-Immersive-Visual-Toolkit/UnityOculusAndroidVRBrowser"
            },
            "thumbnailUrl": "images/chi-thumb.gif",
            "imageUrl": "",
            "abstract": "Mutual awareness of visual attention is crucial for successful collaboration. Previous research has explored various ways to represent visual attention, such as field-of-view visualizations and cursor visualizations based on eye-tracking, but these methods have limitations. Verbal communication is often utilized as a complementary strategy to overcome such disadvantages. This paper proposes a novel method that combines verbal communication with the Cone of Vision to improve gaze inference and mutual awareness in VR. We conducted a within-group study with pairs of participants who performed a collaborative analysis of data visualizations in VR. We found that our proposed method provides a better approximation of eye gaze than the approximation provided by head direction. Furthermore, we release the first collaborative head, eyes, and verbal behaviour dataset. The results of this study provide a foundation for investigating the potential of verbal communication as a tool for enhancing visual cues for joint attention.",
            "publicationDate": ""
        },
        "publication2": {
            "title":"Cone of Vision as a Behavioural Cue for VR Collaboration",
            "venue": "ACM Conference on Computer-Supported Cooperative Work and Social Computing (CSCW 2022)",
            "authors": [
                "Riccardo Bovo",
                "Daniele Giunchi",
                "Muna Alebri",
                "Anthony Steed",
                "Enrico Costanza",
                "Thomas Heinis"
            ],
            "links": {
                "acm": "https://dl.acm.org/doi/abs/10.1145/3555615",
                "youtube": "https://www.youtube.com/watch?v=xbBbSSBMseg",
                "github": "https://github.com/Collaborative-Immersive-Visual-Toolkit/UnityOculusAndroidVRBrowser"
            },
            "thumbnailUrl": "images/cscw-thumb.jpg",
            "imageUrl": "images/cscw-image.png",
            "abstract": "Mutual awareness of visual attention is essential for collaborative work. In the field of collaborative virtual environments (CVE), it has been proposed to use Field-of-View (FoV) frustum visualisations as a cue to support mutual awareness during collaboration. Recent studies on FoV frustum visualisations focus on asymmetric collaboration with AR/VR hardware setups and 3D reconstructed environments. In contrast, we focus on the general-purpose CVEs (i.e., VR shared offices), whose popularity is increasing due to the availability of low-cost headsets, and the restrictions imposed by the pandemic. In these CVEs collaboration roles are symmetrical, and the same 2D content available on desktop computers is displayed on 2D surfaces in a 3D space (VR screens). We prototyped one such CVE to evaluate FoV frustrum visualisation within this collaboration scenario. We also implement a FoV visualisation generated from an average fixation map (AFM), therefore directly generated by users' gaze behaviour which we call Cone of Vision (CoV). Our approach to displaying the frustum visualisations is tailored for 2D surfaces in 3D space and allows for self-awareness of this visual cue. We evaluate CoV in the context of a general exploratory data analysis (EDA) with 10 pairs of participants. Our findings indicate that CoV is beneficial during shifts between independent and collaborative work and supports collaborative progression across the visualisation. Self-perception of the CoV improves visual attention coupling, reduces the numbe",
            "publicationDate": ""
        },
        "publication3": {
            "title": "Real-time head-based deep-learning model for gaze probability regions in collaborative VR",
            "venue": "ACM Symposium on Eye Tracking Research and Applications (ETRA 2022)",
            "authors": [
                "Riccardo Bovo",
                "Daniele Giunchi",
                "Ludwig Sidenmark",
                "Hans Gellersen",
                "Enrico Costanza",
                "Thomas Heinis"
            ],
            "links": {
                "acm": "https://dl.acm.org/doi/abs/10.1145/3517031.3529642",
                "youtube": "https://youtu.be/RPv-FU2GqN0",
                "github": "https://github.com/Collaborative-Immersive-Visual-Toolkit/VR_Iso_Gaze_Contours"
            },
            "thumbnailUrl": "images/etra-thumb.gif",
            "imageUrl": "images/etra-image.gif",
            "abstract": "Eye behavior has gained much interest in the VR research community as an interactive input and support for collaboration. Researchers used head behavior and saliency to implement gaze inference models when eye-tracking is missing. However, these solutions are resource-demanding and thus unfit for untethered devices, and their angle accuracy is around 7°, which can be a problem in high-density informative areas. To address this issue, we propose a lightweight deep learning model that generates the probability density function of the gaze as a percentile contour. This solution allows us to introduce a visual attention representation based on a region rather than a point. In this way, we manage the trade-off between the ambiguity of a region and the error of a point. We tested our model in untethered devices with real-time performances; we evaluated its accuracy, outperforming our identified baselines (average fixation map and head direction).",
            "publicationDate": ""
        },
        "publication4": {
            "title": "Detecting errors in pick and place procedures: detecting errors in multi-stage and sequence-constrained manual retrieve-assembly procedures",
            "venue": "ACM Conference on Intelligent User Interfaces (IUI 2020)",
            "authors": [
                "Riccardo Bovo",
                "Nicola Binetti",
                "Duncan P Brumby",
                "Simon Julier"
            ],
            "links": {
                "acm": "https://dl.acm.org/doi/abs/10.1145/3377325.3377497"
            },
            "thumbnailUrl": "images/error-pred.jpg",
            "imageUrl": "",
            "abstract": "Many human activities, such as manufacturing and assembly, are sequence-constrained procedural tasks (SPTs): they consist of a series of steps that must be executed in a specific spatial/temporal order. However, these tasks can be error prone - steps can be missed out, executed out-of-order, and repeated. The ability to automatically predict if a person is about to commit an error could greatly help in these cases. The prediction could be used, for example, to provide feedback to prevent mistakes or mitigate their effects. In this paper, we present a novel approach for real-time error prediction for multi-step sequence tasks which uses a minimum viable set of behavioural signals. We have three main contributions. The first we present an architecture for real-time error prediction based on task tracking and intent prediction. The second is to explore the effectiveness of using hand position and eye-gaze tracking for task tracking. We confirm that eye-gaze is more effective for intent prediction, hand tracking is more accurate for task tracking and that combining the two provides the best overall response. We show that using Hands and Gaze tracking data we can predict selection/placement errors with an F1 score of 97%, approximately 300ms before the error would occur. Finally, we discuss the application of this hand-gaze error detection architecture used in conjunction with head-mounted AR displays, to support industrial manual assembly.",
            "publicationDate": ""
        },
        "publication5": {
            "title": "MR-RIEW: An MR Toolkit for Designing Remote Immersive Experiment Workflows",
            "venue" : "IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR 2022)",
            "authors": [
                "Riccardo Bovo",
                "Daniele Giunchi",
                "Anthony Steed",
                "Thomas Heinis"
            ],
            "links": {
                "ieee": "https://ieeexplore.ieee.org/document/9757432",
                "github": "https://github.com/CrowdVRLab/MR-RIEW",
                "youtube": "https://youtu.be/Ra9uwnenAxE"
            },
            "thumbnailUrl": "images/mr-riew.gif",
            "imageUrl": "",
            "abstract": "We present MR-RIEW, a toolkit for virtual and mixed reality that provides researchers with a dynamic way to design an immersive experiment workflow including instructions, environments, sessions, trials and questionnaires. It is implemented in Unity via scriptable objects, allowing simple customisation. The graphic elements, the scenes and the questionnaires can be selected and associated without code. MR-RIEW can save locally into the headset and remotely the questionnaire's answers. MR-RIEW is connected to Google Firebase service for the remote solution requiring a minimal configuration.",
            "publicationDate": ""
        },
        "publication6": {
            "title": "Mitigation strategies for participant non-attendance in VR remote collaborative experiments",
            "venue": "Frontiers in Computer Science, 2022",
            "authors": [
                "Riccardo Bovo",
                "Daniele Giunchi",
                "Enrico Costanza",
                "Anthony Steed",
                "Thomas Heinis"
            ],
            "links": {
                "frontiers": "https://www.frontiersin.org/articles/10.3389/fcomp.2022.928269/full"
            },
            "thumbnailUrl": "images/frontiers.jpg",
            "imageUrl": "",
            "abstract": "COVID-19 led to the temporary closure of many HCI research facilities disrupting many ongoing user studies. While some studies could easily move online, this has proven problematic for virtual reality (VR) studies. The main challenge of remote VR study is the recruitment of participants who have access to specialized hardware such as head-mounted displays. This challenge is exacerbated in collaborative VR studies, where multiple participants need to be available and remotely connect to the study simultaneously. We identify the latter as the worst-case scenario regarding resource wastage and frustration. Across two collaborative user studies, we identified the personal connection between the experimenter and the participant as a critical factor in reducing non-attendance. We compare three recruitment strategies that we have iteratively developed based on our recent experiences. We introduce a metric to quantify the cost for each recruitment strategy, and we show that our final strategy achieves the best metric score. Our work is valuable for HCI researchers recruiting participants for collaborative VR remote studies, but it can be easily extended to every remote experiment scenario.",
            "publicationDate": ""
        },
        "publication7": {
            "title": "Shall I describe it or shall I move closer? Verbal references and locomotion in VR collaborative search tasks",
            "venue": "European Conference on Computer-Supported Cooperative Work (ECSCW 2021)",
            "authors": [
                "Riccardo Bovo",
                "Daniele Giunchi",
                "Enrico Costanza",
                "Anthony Steed",
                "Thomas Heinis"
            ],
            "links": {
                "eusset": "https://dl.eusset.eu/handle/20.500.12015/4376"
            },
            "thumbnailUrl": "images/ecscw.gif",
            "imageUrl": "",
            "abstract": "Research in pointing-based communication within immersive collaborative virtual environments (ICVE) remains a compelling area of study. Previous studies explored techniques to improve accuracy and reduce errors when hand-pointing from a distance. In this study, we explore how users adapt their behaviour to cope with the lack of accuracy during pointing. In an ICVE where users can move (i.e., locomotion) when faced with a lack of laser pointers, pointing inaccuracy can be avoided by getting closer to the object of interest. Alternatively, collaborators can enrich the utterances with details to compensate for the lack of pointing precision. Inspired by previous CSCW remote desktop collaboration, we measure visual coordination, the implicitness of deixis’ utterances and the amount of locomotion. We design an experiment that compares the effects of the presence/absence of laser pointers across hard/easy-to-describe referents. Results show that when users face pointing inaccuracy, they prefer to move closer to the referent rather than enrich the verbal reference.",
            "publicationDate": ""
        },
        "publication8": {
            "title": "Perceived realism of pedestrian crowds trajectories in vr",
            "venue": "ACM Symposium on Virtual Reality Software and Technology (VRST 2021)",
            "authors": [
                "Daniele Giunchi",
                "Riccardo Bovo",
                "Panayiotis Charalambous",
                "Fotis Liarokapis",
                "Alastair Shipman",
                "Stuart James",
                "Anthony Steed",
                "Thomas Heinis"
            ],
            "links": {
                "acm": "https://dl.acm.org/doi/abs/10.1145/3489849.3489860",
                "github": "https://github.com/CrowdVRLab/track_pedestrian",
                "youtube": "https://youtu.be/J1jHbYEzzb4"
            },
            "imageUrl": "",
            "thumbnailUrl": "images/pedestrian.gif",
            "abstract": "Crowd simulation algorithms play an essential role in populating Virtual Reality (VR) environments with multiple autonomous humanoid agents. The generation of plausible trajectories can be a significant computational cost for real-time graphics engines, especially in untethered and mobile devices such as portable VR devices. Previous research explores the plausibility and realism of crowd simulations on desktop computers but fails to account the impact it has on immersion. This study explores how the realism of crowd trajectories affects the perceived immersion in VR. We do so by running a psychophysical experiment in which participants rate the realism of real/synthetic trajectories data, showing similar level of perceived realism.",
            "publicationDate": ""
        },
        "publication9": {
            "title": "Explorative Study on Asymmetric Sketch Interactions for Object Retrieval in Virtual Reality",
            "venue": "International Conference on Interactive Media, Smart Systems and Emerging Technologies (IMET 2022)",
            "authors": [
                "Daniele Giunchi",
                "Stuart James",
                "Riccardo Bovo",
                "Donald Degraen",
                "Anthony Steed"
            ],
            "links": {
                "ieee": "https://ieeexplore.ieee.org/abstract/document/9929445"
            },
            "thumbnailUrl": "images/sketch.jpg",
            "abstract": "Drawing tools for Virtual Reality (VR) enable users to model 3D designs from within the virtual environment itself. These tools employ sketching and sculpting techniques known from desktop-based interfaces and apply them to hand-based controller interaction. While these techniques allow for mid-air sketching of basic shapes, it remains difficult for users to create detailed and comprehensive 3D models. Our work focuses on supporting the user in designing the virtual environment around them by enhancing sketch-based interfaces with a supporting system for interactive model retrieval. An immersed user can query a database containing detailed 3D models and replace them with the virtual environment through sketching. To understand supportive sketching within a virtual environment, we made an explorative comparison between asymmetric methods of sketch interaction, i.e., 3D mid-air sketching, 2D sketching on a virtual tablet, 2D sketching on a fixed virtual whiteboard, and 2D sketching on a real tablet. Our work shows that different patterns emerge when users interact with 3D sketches rather than 2D sketches to compensate for different results from the retrieval system. In particular, the user adopts strategies when drawing on canvas of different sizes or using a physical device instead of a virtual canvas. While we pose our work as a retrieval problem for 3D models of chairs, our results can be extrapolated to other sketching tasks for virtual environments.",
            "publicationDate": ""
        },
        "publication10": {
            "title": "A taxonomy for combining activity recognition and process discovery in industrial environments",
            "venue": "International Conference on Intelligent Data Engineering and Automated Learning",
            "authors": [
                "Felix Mannhardt",
                "Riccardo Bovo",
                "Manuel Fradinho Oliveira",
                "Simon Julier"
            ],
            "links": {
                "springer": "https://link.springer.com/chapter/10.1007/978-3-030-03496-2_10"
            },
            "thumbnailUrl": "images/tax-thumb.jpg",
            "abstract": "Despite the increasing automation levels in an Industry 4.0 scenario, the tacit knowledge of highly skilled manufacturing workers remains of strategic importance. Retaining this knowledge by formally capturing it is a challenge for industrial organisations. This paper explores research on automatically capturing this knowledge by using methods from activity recognition and process mining on data obtained from sensorised workers and environments. Activity recognition lifts the abstraction level of sensor data to recognizable activities and process mining methods discover models of process executions. We classify the existing work, which largely neglects the possibility of applying process mining, and derive a taxonomy that identifies challenges and research gaps.",
            "publicationDate": ""
        },
        "publication11": {
            "title": "EmBARDiment: an Embodied AI Agent for Productivity in XR",
            "venue": "arXiv preprint 2024",
            "authors": [
                "Riccardo Bovo",
                "S Abreu",
                "K Ahuja",
                "E J Gonzalez",
                "L T Cheng",
                "M Gonzalez-Franco"
            ],
            "links": {
                "arxiv": "https://arxiv.org/abs/2408.08158",
                "youtube": "https://youtu.be/vQnOxPi3qpQ"
            },
            "thumbnailUrl": "images/embardiment-thumbs.jpg",
            "imageUrl": "images/embardiment-image.jpg",
            "abstract": "XR devices running chat-bots powered by Large Language Models (LLMs) have tremendous potential as always-on agents that can enable much better productivity scenarios. However, screen based chat-bots do not take advantage of the the full-suite of natural inputs available in XR, including inward facing sensor data, instead they over-rely on explicit voice or text prompts, sometimes paired with multi-modal data dropped as part of the query. We propose a solution that leverages an attention framework that derives context implicitly from user actions, eye-gaze, and contextual memory within the XR environment. This minimizes the need for engineered explicit prompts, fostering grounded and intuitive interactions that glean user insights for the chat-bot. Our user studies demonstrate the imminent feasibility and transformative potential of our approach to streamline user interaction in XR with chat-bots, while offering insights for the design of future XR-embodied LLM agents.",
            "publicationDate": ""
        },
        "publication12": {
            "title": "Visualisations with Semantic Icons: Assessing Engagement with Distracting Elements",
            "venue": "International Journal of Human-Computer Studies, 2024",
            "authors": [
                "Muna Alebri",
                "Enrico Costanza",
                "Georgia Panagiotidou",
                "Duncan P Brumby",
                "Farah Althani",
                "Riccardo Bovo"
            ],
            "links": {
                "doi": "https://www.sciencedirect.com/science/article/pii/S1071581924001265"
            },
            "thumbnailUrl": "images/semantic.jpg",
            "abstract": "As visualisations reach a broad range of audiences, designing visualisations that attract and engage becomes more critical. Prior work suggests that semantic icons entice and immerse the reader; however, little is known about their impact with informational tasks and when the viewer’s attention is divided because of a distracting element. To address this gap, we first explored a variety of semantic icons with various visualisation attributes. The findings of this exploration shaped the design of our primary comparative online user studies, where participants saw a target visualisation with a distracting visualisation on a web page and were asked to extract insights. Their engagement was measured through three dependent variables: (1) visual attention, (2) effort to write insights, and (3) self-reported engagement. In Study 1, we discovered that visualisations with semantic icons were consistently perceived to be more engaging than the plain version. However, we found no differences in visual attention and effort between the two versions. Thus, we ran Study 2 using visualisations with more salient semantic icons to achieve maximum contrast. The results were consistent with our first Study. Furthermore, we found that semantic icons elevated engagement with visualisations depicting less interesting and engaging topics from the participant’s perspective. We extended prior work by demonstrating the semantic value after performing an informational task (extracting insights) and reflecting on the visualisation, besides its value to the first impression. Our findings may be helpful to visualisation designers and storytellers keen on designing engaging visualisations with limited resources. We also contribute reflections on engagement measurements with visualisations and provide future directions.",
            "publicationDate": ""
        },
        "publication13": {
            "title": "WindowMirror: An Open-Source Toolkit to Bring Interactive Multi-Window Views into XR",
            "venue": "IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR 2024)",
            "authors": [
                "Riccardo Bovo",
                "E J Gonzalez",
                "L T Cheng",
                "M Gonzalez-Franco"
            ],
            "links": {
                "ieee": "https://ieeexplore.ieee.org/abstract/document/10536416",
                "github": "https://github.com/google/window-mirror"
            },
            "imageUrl": "",
            "thumbnailUrl": "images/windowmirror-thumb.gif",
            "abstract": "WindowMirror toolkit provides users with a simulated, extended screen real-estate. It allows users to interact with multiple desktop applications in real-time within a XR environment. Our architecture has two main modules: a Unity package and a Python backend. WindowMirror supports traditional desktop interaction methods such as mouse, keyboard and it features a Cylindrical Window Layout, an emerging design pattern which is particularly effective for single- user, egocentric perspectives.",
            "publicationDate": ""
        },
        "publication14": {
            "title": "Streamspace: A Framework for Window Streaming in Collaborative Mixed Reality Environments",
            "venue": "IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR 2024)",
            "authors": [
                "Daniele Giunchi",
                "Riccardo Bovo",
                "N Numan",
                "Anthony Steed"
            ],
            "links": {
                "ieee": "https://ieeexplore.ieee.org/document/10536446",
                "github": "https://github.com/UCL-VR/streamspace"
            },
            "thumbnailUrl": "images/streamspace-thumb.jpg",
            "imageUrl": "images/streamspace-image.png",
            "abstract": "We introduce Streamspace as a framework for exploring screen based collaborative MR experiences, focusing on the streaming, integration, and layout of screen content in MR environments. Utilizing Unity and Ubiq, this framework allows users to engage with, reposition, and resize uniquely identified screens within a user-centric virtual space. Building on Ubiq's WebRTC capabilities, our frame work enables real-time streaming and transformations through peer-to-peer communication. Key features of Streamspace include distributed streaming, automated screen layout, and flexible privacy settings for virtual screens. Introducing Streamspace, we aim to provide a foundational basis for research on screen-based collaborative MR applications.",
            "publicationDate": ""
        }
    }
}